---
title: 'Explaining or Predicting Graduation Rates Using IPEDS'
subtitle: "ECI 586 Introduction to Learning Analytics"
author: "Dr. Shaun Kellogg"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
set.seed(20240706) # so the results are readily reproducible
```

## 1. PREPARE

In Unit 2, we learn about five basic steps in a supervised machine learning process in addition to some other components of a learning analytics workflow. For example, to help prepare for analysis, we'll first take a step back and think about how we want to use machine learning, and *predicting* is a key word. Many scholars have focused on predicting students who are *at-risk*: of dropping a course or not succeeding in it. In this introductory machine learning case study, we will cover the following workflow processes from @krumm2018 as we attempt to develop our own model for predicting student drop-out:

1.  **Prepare**: Prior to analysis, we'll look at the context from which our data came, formulate a basic research question, and get introduced the {tidymodels} packages for machine learning.

2.  **Wrangle**: Wrangling data entails the work of cleaning, transforming, and merging data. In Part 2 we focus on importing CSV files and modifying some of our variables.

3.  **Explore**: We take a quick look at our variables of interest and do some basic "feature engineering" by creating some new variables we think will be predictive of students at risk.

4.  **Model:** We introduce five basic steps in a supervised machine learning process, focusing on the mechanics of **making predictions**.

5.  **Communicate:** To wrap up our case study, we'll create our first "data product" and share our analyses and findings by creating our first web page using R Markdown.

### 1a. Conceptual Focus

Conceptually, in this case study we focus on prediction, the primary goal of supervised machine learning, and how it differs from the goals of description or explanation, a goal of traditional statistical methods. The reading introduced below focuses on this distinction between prediction and description or explanation. It is one of the most widely-read papers in machine learning and articulates how machine learning differs from other kinds of statistical models. Breiman describes the difference in terms of *data modeling* (models for description and explanation) and *algorithmic modeling* (what we call prediction or machine learning models).

#### Research Question

Technically, we'll focus on the core parts of doing a machine learning analysis in R. We'll use the {[tidymodels](https://www.tidymodels.org/)} set of R packages (add-ons) to do so. We use as recent study by Zong and Davis @zong2024modeling as an inspiration for ours. These authors used inferential models to try to understand what relates to the graduation rates of around 700 four-year universities in the United States, predicting this outcome on the basis of student background, finance, academic and social environment, and retention rate independent variables. You can find this study in the `lit` folder if you are interested in taking a look.

However, to help anchor our analysis and provide us with some direction, we'll focus on the following research question as we explore this new data set:

> How well can we predict drop out rates for among four-year universities?

#### Literature Review

![](img/breiman.png){width="50%"}

Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). *Statistical Science, 16*(3), 199-231. <https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf>

**Abstract**

There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.

#### **👉 Your Turn** **⤵**

You can find this study in the `lit` folder as well. Open up the article and take quick scan of the article and note two observations you have about the article.

-   OBSERVATION 1
-   OBSERVATION 2

### 1b. Load Libraries

#### tidymodels 📦

![](img/tidymodels.svg){fig-align="left" width="30%"}

The [tidymodels](https://www.tidymodels.org) package is a "meta-package" for modeling and statistical analysis that shares the underlying design philosophy, grammar, and data structures of the [tidyverse](https://www.tidyverse.org/). Like the {tidyverse} package, it includes a core set of packages that are loaded on startup and contains tools for:

-   data splitting and pre-processing;
-   model selection, tuning, and evaluation;
-   feature selection and variable importance estimation;
-   as well as other functionality.

#### [**Your Turn**]{style="color: green;"} **⤵**

In addition to the {tidymodels} package, we'll also be using the {tidyverse} packages we learned about in Unit 1, as well as the {janitor} package for quickly cleaning up variable names.

Use the code chunk below to load these three packages:

```{r}
library(tidymodels)
library(tidyverse)
library(janitor)
```

::: callout-tip
Remember to use the `library()` function to load these packages. After you've done that, click the green arrow to run the code chunk. If you see a bunch of messages (not anything labeled as an error), you are good to go! These messages mean the packages loaded correctly.
:::

### 1c. Import & Inspect Data

In this case study, we will be using a new data set from the [IPEDS, the Integrated Postsecondary Education Data System](https://nces.ed.gov/ipeds/) and similar to the data set used by @zong2024modeling.

Run the following code to read in the `ipeds-all-title-9-2022-data.csv` file using the `read_csv()` function, paying attention to where those files are located relative to this case study file – in the data folder!

```{r}
ipeds <- read_csv("data/ipeds-all-title-9-2022-data.csv")
```

We'll then use a handy function from janitor, `clean_names()`. It does what it seems like it should - it cleans the names of variables, making them easy to view and type. Run this next code chunk.

```{r}
ipeds <- janitor::clean_names(ipeds)
```

#### **👉 Your Turn** **⤵**

In the chunk below, examine the data set using a function or means of your choice (such as just *printing* the data set by typing its name or using the `glimpse()` function). Do this in the code chunk below! Note its dimensions --- especially how many rows it has!

```{r}
ipeds
```

Now write down a couple observations after inspecting the data - and any all observations welcome!

-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE

#### ❓Question

Recall that similar to @zong2024modeling, we are trying to predict student drop out rates using data readily available to higher education researchers. Take a look at our data set again and list three variables you think might be useful for predicting graduation rates:

-   VARIABLE 1
-   VARIABLE 2
-   VARIABLE 3

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning, reshaping, transforming, and merging data @wickham2023r. The importance of data wrangling is difficult to overstate, as it involves the initial steps of going from raw data to a dataset that can be explored and modeled @krumm2018. In Part 2, we focus on the the following wrangling processes to:

a.  **Selecting Variables**. We use the `select()` function to simultaneous select variables for analysis and rename exceptionally long variables.

b.  **Filtering Variables.** We use the `filter()` function to further reduce our data set to include only Title IV postsecondary institutions.

### 2a. Select Variables

Even though we cleaned the names to make them easier to view and type (thanks, `clean_names()`)), they are still pretty long.

The code chunk below uses a very handy function, `select()`. This allows you to simultaneously choose and rename variables, returning a data frame with only the variables you have selected --- named as you like. For now, we'll just run this code. Later in your analyses, you'll almost certainly use `select()` to get a more manageable dataset.

```{r}
ipeds <- ipeds |> 
    select(name = institution_name, 
           title_iv = hd2022_postsecondary_and_title_iv_institution_indicator,
           carnegie_class = hd2022_carnegie_classification_2021_basic,
           state = hd2022_state_abbreviation,
           total_enroll = drvef2022_total_enrollment,
           pct_admitted = drvadm2022_percent_admitted_total,
           n_bach = drvc2022_bachelors_degree,
           n_mast = drvc2022_masters_degree,
           n_doc = drvc2022_doctors_degree_research_scholarship,
           tuition_fees = drvic2022_tuition_and_fees_2021_22,
           grad_rate = drvgr2022_graduation_rate_total_cohort,
           percent_fin_aid = sfa2122_percent_of_full_time_first_time_undergraduates_awarded_any_financial_aid,
           avg_salary = drvhr2022_average_salary_equated_to_9_months_of_full_time_instructional_staff_all_ranks)
```

Before moving on, let's make sure we understand what each variables represents. Even though the variables names are fairly intuitive, below is a breif description of variables we just selected:

-   name: Institution name

-   **title_iv**: Indicator if the university is Title IV eligible

-   **carnegie_class**: Carnegie Classification of the institution

-   **state**: State abbreviation

-   **total_enroll**: Total enrollment

-   **pct_admitted**: Percentage of applicants admitted

-   **n_bach**: Number of students receiving a bachelor’s degree

-   **n_mast**: Number receiving a master’s degree

-   **n_doc**: Number receiving a doctoral degree

-   **tuition_fees**: Total cost of tuition and fees

-   **grad_rate**: Graduation rate

-   **percent_fin_aid**: Percent of students receiving financial aid

-   **avg_salary**: Average salary of instructional staff

Sometimes publicly available data sets, particularly high-quality ones like IPEDS, will have a **codebook** or **glossary** to help that provides detailed information about a dataset, serving as a guide to understanding the structure, contents, and variables within the data. It essentially acts as a “dictionary” for your dataset, helping researchers, analysts, and anyone else using the data to interpret it correctly.

#### **👉 Your Turn** **⤵**

Visit the the IPEDS glossary located here: <https://surveys.nces.ed.gov/ipeds/public/glossary>

Use the glossary to look up a variable from above or in our larger data set that you are interested in understanding better and record the full definition below:

-   DEFINITION

::: callout-note
**Fun fact**: RTI International – located in the North Carolina's Research Triangle Park and from which they derive their name – has led IPEDS for over 20 years, collecting institution-level data from primary providers of postsecondary education nationwide. To learn more about their work visit: <https://www.rti.org/impact/integrated-postsecondary-education-data-system-ipeds>
:::

### 2b. Count Variables

As illustrated in the figure below, @krumm2018 noted in Chapter 2 of *Learning Analytics goes to Schools,* processes in the Data-Intensive Research Workflow workflow (e.g., preparing, wrangling, exploring, etc.) are more can be seens as overlapping activities as much as distinct sequentional steps.

![](img/workflow-overlap.png){width="80%"}

For example, next we will explore our data a little bit to assist with our data wrangling process.

A useful function for exploring data is `count()`; it does what it sounds like! It counts how many times values for a variable appear.

```{r}
ipeds |> 
    count(title_iv)
```

This suggests we may wish to filter the 30 non-Title IV institutions --- something we'll do shortly.

#### **👉 Your Turn** **⤵**

Can you count another variable? Pick another (see the code chunk two above) and add a count below. While simple, counting up different values in our data can be very informative (and can often lead to further explorations)!

```{r}
ipeds |> 
    count(carnegie_class)
```

### 2c. Filter Variables

Our final data wrangling step is filtering our data set to include only Title IV postsecondary institutions.

We'll do this with a function you should now be fairly familiar with. `filter()` is a very handy function that is part of the tidyverse; it filters to *include* (or *exclude*) observations in your data based upon logical conditions (e.g., `==`, `>`, `<=`, etc.). See more [here](https://dplyr.tidyverse.org/reference/filter.html) if interested.

Run the code chunk below to filter the data so it only includes only Title IV postsecondary institutions.

```{r}
ipeds <- ipeds |> 
    filter(title_iv == "Title IV postsecondary institution")
```

#### **👉 Your Turn** **⤵**

Can you filter the data again, this time to *only* include institutions with a carnegie classification?

In other words, can you exclude those institutions with a value for the `carnegie_class` variable that is "Not applicable, not in Carnegie universe (not accredited or nondegree-granting)")? A little hint: whereas the logical operator `==` is used to include only matching conditions, the logical operator `!=` excludes matching conditions.

```{r}
ipeds <- ipeds |> 
    filter(carnegie_class != "Not applicable, not in Carnegie universe (not accredited or nondegree-granting)")
```

#### **👉 Your Turn** **⤵**

We're cruising! Let's take another peak at our data - using `glimpse()` or another means of your choosing below.

```{r}
glimpse(ipeds)
```

## 3. EXPLORE

As noted by @krumm2018, exploratory data analysis often involves some combination of data visualization and feature engineering. In Part 3, we will create some quick visualization to help us better understand our data and transform our dependent variable from continuous to categorical to simplify our modeling. Specifically, in Part 3 we will:

a.  **Visualize Variables** by using the {ggplot2} pacakge to visually inspect our `grad_rate` dependent variable as well as other variables of interest.

b.  **Dichotomize Dependent Variable** by "mutating" our `grad_rate` dependent variable from our continuous variable to a dichotomous variable.

### 3a. Examine Dependent Variable

One key step in most analyses is to explore the data. Here, we conduct an exploratory data analysis with the IPEDS data, focusing on the key outcome of graduate rate.

Below, we use the ggplot2 package (part of the tidyverse) to visualize the *spread* of the values of our dependent variable, `grad_rate`, which represents institutions' graduation rate. There is a *lot* to ggplot2, and data visualizations are not the focus of this module, but [this web page](https://ggplot2.tidyverse.org/) has a lot of information you can use to learn more, if you are interested. ggplot2 is fantastic for creating publication-ready visualizations!

```{r}
ipeds |> 
    ggplot(aes(x = grad_rate)) +
    geom_histogram()
```

#### ❓Question

What do you notice about this graph -- and about graduation rate?

-   YOUR RESPONSE
-   YOUR RESPONSE

#### **👉 Your Turn** **⤵**

Below, can you add one ggplot2 plot with a different variable/variables? Use the ggplot2 page linked above (also [here](https://ggplot2.tidyverse.org/)) or the code above as a starting point (another histogram is fine!) for your visualization.

```{r}
ipeds |> 
    ggplot(aes(x = pct_admitted)) +
    geom_histogram()

ipeds |> 
    ggplot(aes(x = tuition_fees, y = total_enroll, color = avg_salary)) +
    geom_point()
```

### 3b. Dichotomoize Variables

Next we'll again overlap our data wrangling and exploring activities in preparation for some data modeling that will come next. Recall that we are interested in assessing how we we can predict graduation rates, our **dependent variable**, using other variables or **predictors** in our data set.

Supervised machine learning, or predictive modeling, involves two broad approaches: classification and regression.

-   **Classification** algorithms model categorical outcomes (e.g., yes or no outcomes);

-   **Regression** algorithms characterize continuous outcomes (e.g., test scores).

To simplify our analysis for this case study, we'll focus on classification. Specifically, we will model our dependent variable, `grad_rate`, as a dichotomous (i.e., yes or no; 1 or 0) dependent variable. This isn't necessary, but it makes the contrast between the regression and supervised machine learning model a bit more vivid, and also dichotomous and categorical outcome variables are common in supervised machine learning applications, and so we'll do this for this case study.

#### **👉 Your Turn** **⤵**

Your next task is to decide what constitutes a good graduation rate. Our only suggestion - don't pick a number *too* close to 0% or 100%. Otherwise, please replace XXX below with the number from 0-100 that represents the graduation rate percentage. Just add the number --- don't add the percentage symbol.

```{r}
ipeds <- ipeds |> 
    mutate(good_grad_rate = if_else(grad_rate > 66, 1, 0),
           good_grad_rate = as.factor(good_grad_rate))
```

Here, add a reason or two for how and why you picked the number you did:

-   REASON 1

-   REASON 2

Before moving on, let's unpack what we just did in the few lines of code above. The code is performing two primary operations on our `ipeds` dataset using the `mutate()` function from the `dplyr` package:

1.  **Creating a New Variable `good_grad_rate`:**
    -   The `mutate()` function adds a new variable named `good_grad_rate` to the `ipeds` dataset.
    -   `if_else(grad_rate > XXX, 1, 0)`: This part creates a binary (0 or 1) variable based on the `grad_rate` column.
        -   If `grad_rate` is greater than `XXX` (where `XXX` is a placeholder value that needs to be specified), the value of `good_grad_rate` will be `1`.
        -   If `grad_rate` is not greater than `XXX`, the value of `good_grad_rate` will be `0`.
2.  **Converting `good_grad_rate` to a Factor:**
    -   `good_grad_rate = as.factor(good_grad_rate)`: This part converts the `good_grad_rate` variable from a numeric type (0 or 1) to a factor type. Factors are useful in R for categorical data, and they help in modeling processes where the variable is treated as a categorical predictor or outcome variable.

In summary, the code is creating a new categorical variable `good_grad_rate` in the `ipeds` dataset that indicates whether the graduation rate (`grad_rate`) is above a specified threshold (`XXX`) and then converts this variable into a factor type with two levels: `0` (not above the threshold) and `1` (above the threshold).

## 4. MODEL

Recall from our readings that there are two general types of modeling approaches: unsupervised and supervised machine learning. In Part 4, we focus on supervised learning models, which are used to quantify relationships between features (e.g., total enrollments and % of students admitted) and a known outcome (e.g., a good graduation rate). These models can be used for statistical inference, as illustrated in Unit 1, or prediction as we'll illustrate in this section.

In Chapter 3, @krumm2018 note that the term term regression can take on different meanings across inference and prediction uses:

> From a statistical, or inferential perspective, regression denotes a family of models that can be used on either categorical or continuous outcomes. Perhaps most confusing to newcomers or to researchers steeped in either inference or prediction are the ways in which specific models, such as logistic regression, can be used for either inference or classification.

Now, we will proceed to the analyses.

### 4a. Inferential Analysis

We'll first conduct a simple regression analysis from a statistical inference perspective, similar to what we did in Unit 1 for our online learning data.

Run the code chunk below to fit generalized linear model `glm()` function, due to the dependent variable being dichotomous. The code is relatively straightforward; the comments explain each step.

```{r}
m1 <- glm(good_grad_rate ~ 
            total_enroll + 
            pct_admitted + 
            n_bach + 
            n_mast + 
            n_doc + 
            tuition_fees + 
            percent_fin_aid + 
            avg_salary, 
          data = ipeds, 
          family = "binomial") 

summary(m1) 
```

#### Code Explanation

Before interpreting the output of this model, let's breakdown what each line of code above is doing:

1.  **Building a Logistic Regression Model (`glm`):**
    -   `m1 <- glm(...)`: This line creates a generalized linear model (GLM) and assigns it to the object `m1`.
    -   `good_grad_rate ~ ...`: The model is predicting `good_grad_rate`, a binary outcome, using multiple independent variables (`total_enroll`, `pct_admitted`, `n_bach`, `n_mast`, `n_doc`, `tuition_fees`, `percent_fin_aid`, and `avg_salary`).
    -   The `family = "binomial"` argument specifies that this is a logistic regression model, which is appropriate when the dependent variable (`good_grad_rate`) is binary (0 or 1).
2.  **Independent Variables:**
    -   The model includes multiple predictors:
        -   `total_enroll`: Total enrollment
        -   `pct_admitted`: Percentage of applicants admitted
        -   `n_bach`: Number of students receiving a bachelor's degree
        -   `n_mast`: Number receiving a master's degree
        -   `n_doc`: Number receiving a doctoral degree
        -   `tuition_fees`: Total cost of tuition and fees
        -   `percent_fin_aid`: Percentage of students receiving financial aid
        -   `avg_salary`: Average salary of instructional staff
3.  **Displaying the Model Output (`summary(m1)`):**
    -   Finally, the `summary(m1)` function provides detailed information about the logistic regression model, including:
        -   Coefficients for each predictor variable.
        -   Standard errors, z-values, and p-values for hypothesis testing.
        -   Overall model statistics such as the null and residual deviance, indicating model fit.

In summary, our code fits a logistic regression model to help us determine whether a university has a "good" graduation rate (`good_grad_rate`) based on several institutional characteristics. The `summary(m1)` output will show which predictors are statistically significant and how they influence the likelihood of a "good" graduation rate.

#### Output Interpretation

The table below provides the results of a logistic regression model predicting whether a university has a "good" graduation rate (`good_grad_rate`) based on various institutional characteristics. Note that your output will look a little different based on the cut-off point you selected for a "good" graduate rate. In my case, I somewhat arbitratily decided that if at least two-thirds (\~67%) of students graduated, it was considered "good."

Now let's break down the interpretation. The table below provides my estimates for each predictor, along with their standard errors, z-values, and p-values:

| Predictor | Estimate | Std. Error | z value | Pr(\> | z |
|------------|------------|------------|------------|------------|------------|
| (Intercept) | -0.4732 | 0.7276 | -0.650 | 0.515466 | (Not significant) |
| total_enroll | -0.0001054 | 0.00004649 | -2.267 | 0.023415 | \* Significant |
| pct_admitted | -0.01357 | 0.003959 | -3.427 | 0.000610 | \*\*\* Highly significant |
| n_bach | 0.0007563 | 0.0001981 | 3.817 | 0.000135 | \*\*\* Highly significant |
| n_mast | -0.0004373 | 0.0002131 | -2.052 | 0.040140 | \* Significant |
| n_doc | 0.005749 | 0.001520 | 3.782 | 0.000156 | \*\*\* Highly significant |
| tuition_fees | 0.00007245 | 5.630e-06 | 12.868 | \< 2e-16 | \*\*\* Highly significant |
| percent_fin_aid | -0.03674 | 0.006666 | -5.512 | 3.56e-08 | \*\*\* Highly significant |
| avg_salary | 0.00002599 | 4.396e-06 | 5.912 | 3.39e-09 | \*\*\* Highly significant |

First lets take a look at the model coefficients:

-   **Intercept**: The intercept value (-0.4732) represents the log-odds of a university having a “good” graduation rate when all predictor variables are set to zero. The p-value (0.515) indicates that the intercept is not statistically significant, which isn’t a concern because our primary interest is in understanding the relationship between the independent variables and a `good_grad_rate`.
-   **total_enroll**: For each additional unit increase in total enrollment, the log-odds of having a good graduation rate decrease by 0.0001054. This effect is statistically significant (\*).
-   **pct_admitted**: A 1% increase in the percentage of students admitted leads to a decrease in the log-odds of having a good graduation rate by 0.01357. This effect is highly significant (\*\*\*).
-   **n_bach**: Each additional bachelor's degree awarded is associated with an increase in the log-odds of a good graduation rate by 0.0007563, and this effect is highly significant (\*\*\*).
-   **n_mast**: Each additional master's degree awarded is associated with a slight decrease in the log-odds by 0.0004373, and this effect is marginally significant (\*).
-   **n_doc**: Each additional doctoral degree awarded is associated with an increase in the log-odds of a good graduation rate by 0.005749, and this effect is highly significant (\*\*\*).
-   **tuition_fees**: An increase of one unit in tuition fees is associated with a significant increase in the log-odds of having a good graduation rate by 0.00007245 (\*\*\*).
-   **percent_fin_aid**: A 1% increase in students receiving financial aid is associated with a decrease in the log-odds of having a good graduation rate by 0.03674, and this effect is highly significant (\*\*\*).
-   **avg_salary**: Each additional unit in average salary for instructional staff is associated with an increase in the log-odds of having a good graduation rate by 0.00002599, and this effect is highly significant (\*\*\*).

::: callout-note
Significance codes (\*\*\*, \*\*, \*) provide a quick way to understand the strength of the evidence against the null hypothesis for each predictor in a regression model. Here’s a more detailed explanation:

-    **\*\*\* Highly significant (p \< 0.001)**. This indicates a **very strong** evidence against the null hypothesis (which typically states that the predictor has no effect on the outcome variable). A p-value less than 0.001 means there’s less than a 0.1% chance that the observed relationship is due to random chance. Therefore, we have very high confidence that this predictor is indeed related to the outcome variable.

-   **\*\* Significant (p \< 0.01)**. This indicates **strong** evidence against the null hypothesis. A p-value between 0.001 and 0.01 means there’s less than a 1% chance that the observed relationship is due to random chance. We can still be quite confident that this predictor is related to the outcome variable, though not as strong as in the previous category.

-   **\* Marginally significant (p \< 0.05)**. This indicates **moderate** evidence against the null hypothesis. A p-value between 0.01 and 0.05 means there’s less than a 5% chance that the observed relationship is due to random chance. While this predictor may have a real effect on the outcome variable, the evidence is weaker compared to predictors with smaller p-values.
:::

Now let's take a look a key Model Fit Statistic, the **Akaike Information Criterion (AIC)**, which is a metric used to evaluate how well a model fits the data while also considering the complexity of the model. It not only evaluates how well the model explains the data but also penalizes the model for having too many predictors.

Our model has an AIC Value of 1387.5; however, you can’t interpret the AIC value in isolation as “good” or “bad”; instead, you compare it across models.

-   A **lower AIC value** indicates a model that achieves a better balance between goodness-of-fit and simplicity.
-   It means the model provides a good explanation of the data without being overly complex.

If we were add or remove predictors from our model to test other models, the model with the **lowest AIC** would be considered the best among the compared models, as it suggests the optimal combination of fit and simplicity.

Overall, the model appears to be a good fit for predicting a "good" graduation rate, with tuition fees, percent financial aid, and average salary being especially influential factors. Specifically, the predictors `pct_admitted`, `n_bach`, `n_doc`, `tuition_fees`, `percent_fin_aid`, and `avg_salary` are highly significant in predicting whether a university has a good graduation rate.

#### **👉 Your Turn** **⤵**

### 4a. Prediction Analysis

Specifically, in Part 4 we will learn how to:

a.  **Split Data** into a training and test set that will be used to develop a predictive model.

b.  **Create a "Recipe"** for our predictive model and learn how to deal with nominal data that we would like to use as predictors.

c.  **Fit Models** to our training set using logistic regression and random forest models.

d.  **Check Model Accuracy** on our test set to see how well our model can "predict" our outcome of interest.

Then, we'll conduct a supervised machine learning analysis (with a simple but still commonly used model - in fact, the same model we used for the regression, a generalized linear model!). Again, for now, you'll run this code; later, you'll work through each step in detail.

```{r}
my_rec <- recipe(good_grad_rate ~ total_enroll + pct_admitted + n_bach + n_mast + n_doc + tuition_fees + percent_fin_aid + avg_salary, data = ipeds) # same as above; this sets up what predicts the outcome

# specify model
my_mod <-
    logistic_reg() |> # specifies a logistic regression
    set_engine("glm") |> # specifies the specific package used to estimate the logistic regression
    set_mode("classification") # specifies that our outcome is a dichotomous or categorical variable

my_wf <- workflow() |> # this starts a workflow, which will stitch together the steps in our analysis
    add_model(my_mod) |> # adds the model
    add_recipe(my_rec) # adds the recipe

fit_model <- fit(my_wf, ipeds) # fits the model

predictions <- predict(fit_model, ipeds) |> # predicts the value of the dependent variable using only the predictors' values and our trained model
  bind_cols(ipeds) # this adds the original data back to the predicted values, so everything is together

accuracy <- predictions |> # this and the next two steps calculate the accuracy of our predictions
  metrics(truth = good_grad_rate, estimate = .pred_class) |>
  filter(.metric == "accuracy")

accuracy # print the accuracy
```

The key to observe at this point is what is similar and different between the two approaches (regression and supervised machine learning). Both used the same underlying statistical model, but had some stark differences. Add two or more similarities and two or more differences (no wrong answers!) below.

#### **👉 Your Turn** **⤵**

Similarities:

-   XX
-   XX

Differences:

-   

-   

## 5. COMMUNICATE

The final step in the workflow/process is sharing the results of your analysis with wider audience. Krumm et al. (2018) have outlined the following 3-step process for communicating with education stakeholders findings from an analysis:

1.  **Select.** Communicating what one has learned involves selecting among those analyses that are most important and most useful to an intended audience, as well as selecting a form for displaying that information, such as a graph or table in static or interactive form, i.e. a "data product."

2.  **Polish**. After creating initial versions of data products, research teams often spend time refining or polishing them, by adding or editing titles, labels, and notations and by working with colors and shapes to highlight key points.

3.  **Narrate.** Writing a narrative to accompany the data products involves, at a minimum, pairing a data product with its related research question, describing how best to interpret the data product, and explaining the ways in which the data product helps answer the research question.

Now, let's return to our research questions. What did we find? This (especially the supervised machine learning model and its output) is very likely new, and this is meant to elicit initial perceptions, and not the right answer. What did we find for each of your RQs? Add a few thoughts below for each. Focus on what you would communicate about this analysis to a general audience, again, keeping in mind this is based on your very initial interpretations.

### RQ A

-   ADD YOUR RESPONSE HERE

-   ADD YOUR RESPONSE HERE

### RQ

-   ADD YOUR RESPONSE HERE

-   ADD YOUR RESPONSE HERE

### 🧶 Knit & Check ✅

For your SML Module 1 Badge, you will further reflect on and interpret these models, and their distinctions.

Rendered HTML files can be published online through a variety of ways including [Posit Cloud](https://posit.cloud/learn/guide#publish-from-cloud), [RPubs](#0) , [GitHub Pages](#0), [Quarto Pub](#0), or [other methods](#0). The easiest way to quickly publish your file online is to publish directly from RStudio. You can do so by clicking the "Publish" button located in the Viewer Pane after you render your document as illustrated in the screenshot below.

![](img/publish.png)

Congratulations - you've completed this case study! Move on to the badge activity next.
