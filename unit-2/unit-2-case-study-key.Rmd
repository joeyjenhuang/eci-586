---
title: 'Unit 2 Case Study: Predicting Student Achievement Part 2'
author: "Dr. Shaun Kellogg"
date: "September 7, 2021 (updated: `r Sys.Date()`)"
output:
  html_document:
    toc: yes
    toc_depth: '5'
    toc_float: yes
    code_folding: show
    code_download: TRUE
name: ''
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. PREPARE

Recall from Unit 1 that we focused on conducting an analysis that help
identify predictors of student performance in these online courses. This
case study is guided by a foundational study in Learning Analytics that
illustrates how analyses like these can be used develop an early warning
system to educators identify students at risk of failing and intervene
before that happens. The Unit 1 case study will cover the following
workflow topics:

1.  **Prepare**: Prior to analysis, it's critical to understand the
    context and data sources you're working with so you can formulate
    useful and answerable questions. You'll also need to become familiar
    with and load essential packages for analysis.
2.  **Wrangle**: Wrangling data entails the work of manipulating,
    cleaning, transforming, and merging data. In Part 2 we focus on
    importing CSV files, tidying and joining our data.
3.  **Explore**: In Part 3, we use basic data visualization and
    calculate some summary statistics to explore our data and see what
    insight it provides in response to our question.
4.  **Model:** After identifying variable that may be related to student
    performance through exploratory analysis, we'll look at correlations
    and create some simple models of our data using linear regression.
5.  **Communicate:** To wrap up our case study, we'll develop our first
    "data product" and share our analyses and findings by creating our
    first web page using R Markdown.
6.  

### 1a. Review the Literature

Our Unit 1 Case Study is guided by a well-cited publication from two
authors that have made numerous contributions to the field of Learning
Analytics over the years. Although this article is focused on "early
warning systems" in higher education, and where adoption of learning
management systems (LMS) like Moodle and Canvas gained a quicker
foothold, this study is particularly relevant since COVID-19. Many
districts across the county have incorporated a LMS into their remote
instruction and have set up virtual academies likely to continue
post-pandemic. In North Carolina specifically, student disengagement has
become a particular concern among districtrs and the NC Department of
Public Instruction has recently established the [Office of Learning
Recovery &
Acceleration](https://www.dpi.nc.gov/districts-schools/districts-schools-support/office-learning-recovery-acceleration)**.**

[![](img/mining-lms-data.jpeg){width="40%"}](https://doi.org/10.1177/23328584211024261)

Macfadyen, L. P., & Dawson, S. (2010). [Mining LMS data to develop an
"early warning system" for educators: A proof of
concept.](https://www.sciencedirect.com/science/article/pii/S0360131509002486?via%3Dihub)
*Computers & education*,Â *54*(2), 588-599.

#### Abstract

Earlier studies have suggested that higher education institutions could
harness the predictive power of Learning Management System (LMS) data to
develop reporting tools that identify at-risk students and allow for
more timely pedagogical interventions. This paper confirms and extends
this proposition by providing data from an international research
project investigating **which student online activities accurately
predict academic achievement.** Analysis of LMS tracking data from a
Blackboard Vista-supported course identified 15 variables demonstrating
a significant simple correlation with student final grade... Moreover,
**network analysis of course discussion forums** afforded insight into
the development of the student learning community by identifying
disconnected students, patterns of student-to-student communication, and
instructor positioning within the network. This study affirms that
pedagogically meaningful information can be extracted from LMS-generated
student tracking data, and discusses how these findings are informing
the **development of a dashboard-like reporting tool for educators**
that will extract and visualize real-time data on student engagement and
likelihood of success.

#### Data Sources & Analysis

LIWC...

#### **Summary of Key Findings**

1.  Although 13 LMS variables for this course appear to show significant
    correlation with student final grade, it would be erroneous to rely
    too heavily on the predictive power of simple correlations.
2.  Some students are making more effective strategic decisions about
    time use within the virtual class- room that is not adequately
    represented by simple correlations with time online.
3.  Total number of discussion messages posted, total number of mail
    messages sent, and total number of assessments completed were
    significant contributors to students' final grades and indicated
    that some 33% of the variability in student achievement in this
    course can be explained by this combination of student online
    activities within the course site.
4.  One of the highest performing students established a complex network
    that included a core group of high-performing peers, conversely a
    lower performing student's network is primarily comprised of
    low-performing peers.

#### [**Your Turn**]{style="color: green;"} **â¤µ**

Take a quick scan of Table 3 in the article linked above (and also
located in your "lit" folder in the files pane) and in the space below,
answer the following question: Of the 13 LMS variables correlated with
student final grade, which 2-3 do you think will found to significantly
predict final grades?

-   

Now take a quick look at sections 3.2 Mutltiple Regression and 3.3
Logistic Regression and asnwer the following questions: What factors in
the model did ultimately predict final grades? How accurate was this
model in identifying "at risk" students?

-   

### 1b. Define Questions

In this study, exploratory research was undertaken to identify the data
variables that would inform the development of a data visualization tool
for instructors. This involved the extraction of all LMS tracking
variables for selected course sections at The University of British
Columbia, Canada. In so doing, the study aimed to address the following
research questions:

1.  Which LMS tracking data variables correlate significantly with
    student achievement?

2.  How accurately can measures of student online activity in an online
    course site predict student achievement in the course under study?

3.  Can tracking data recording online student communication patterns
    offer pedagogically meaningful insights into development of a
    student learning community?

For our case study, we'll adopt questions 1 & 2 wholesale to guide our
exploratory analysis and modeling, with a special emphasis on time spent
in the LMS. We'll also use analytical approaches and data similar to
those used by @macfadyen2010mining to better understand how LMS,
gradebook, and survey data might be predictive of student performance.

### 1c. Load Libraries

As noted in our Getting Started activity, R uses "packages" and add-ons
that enhance its functionality. One of our first steps in any workflow
is to load packages necessary for data wrangling, analysis, and
reporting. We'll load packages in this section we are already familiar
with from our tutorials and introduce new packages and corresponding
functions throughout the case study.

#### caret ðŸ“¦

TheÂ [`caret`](http://cran.r-project.org/web/packages/caret/index.html)Â package
(short for **C**lassification **A**nd **RE**gression **T**raining) is a
set of functions that attempt to streamline the process for creating
predictive models. The package contains tools for:

-   data splitting
-   pre-processing
-   feature selection
-   model tuning using resampling
-   variable importance estimation
-   as well as other functionality.

There are many different modeling functions in R. Some have different
syntax for model training and/or prediction. The package started off as
a way to provide a uniform interface the functions themselves, as well
as a way to standardize common tasks (such parameter tuning and variable
importance).

```{r}
library(caret)
```

Don't worry if you saw a number of messages: those probably mean that
the tidyverse loaded just fine. Any conflicts you may have seen mean
that functions in these packages you loaded have the same name as
functions in other packages and R will default to function from the last
loaded package unless you specify otherwise.

```{r}
library(tidyverse)
```

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning,
reshaping, transforming, and merging data (Wickham & Grolemund, 2017).
The importance of data wrangling is difficult to overstate, as it
involves the initial steps of going from raw data to a dataset that can
be explored and modeled (Krumm et al, 2018). In Part 2, we focus on the
the following workflow processes:

a.  **Import Data**. In this section, we introduce the `read_csv()`
    function for working with CSV files and revisit some key functions
    for inspecting our data.

b.  **Remove NAs**. We introduce the `separate()` and `clean_names()`
    functions for getting our data nice and tidy, and revisit the
    `mutate()` for creating new variables.

c.  **Select Predictors**. We conclude our data wrangling by introducing
    `join()`Â functions for merging our processed files into a single
    data frame for analysis.

### a. Import Data

To help us import our data, we'll be using two packages:
{[readr](https://readr.tidyverse.org)} and
{[here](https://here.r-lib.org)} . The {readr} package provides a "fast
and friendly way" to read rectangular data stored in plain-text file
formats like csv, tsv, and fwf. If you are new to readr, I highly
recommend the [data import
chapter](https://r4ds.had.co.nz/data-import.html)Â in R for data science.
We don't need to load the {readr} package because it was already loaded
as part of the tidyverse package we called earlier.

Let's use the `read_csv()` function from {readr} to import our
`log-data.csv` file directly from our data folder and name this data set
`time_spent`, to help us to quickly recollect what function it serves in
this analysis:

```{r}
sci_mo_with_text <- read_csv("data/sci-mo-with-text.csv")
```

#### [**Your Turn**]{style="color: green;"} **â¤µ**

Now use the code chunk below to take a look at the data you just
imported using your preferred methods.

```{r, eval=FALSE}
sci_mo_with_text
```

Remember, if you use the view function to look at your data, your R
Markdown file will not knit so you will need to set the code chunk to
`eval=FALSE`

In the space below, answer the following questions:

1.  How many observations and variables are in this dataset?
2.  What are some new variables that have been introduced from our
    previous case study and what do they represent? **Hint:** Take a
    look at the our Data Science in Ed course text.
3.  Is there any observations missing data for one or more variables?
    How do you know?
4.  Thinking back on our case study from last week, which variables do
    you think

### b. Select Predictors

As is the case with many datasets you'll work with in education
contexts, there is lots of great information in this dataset - but we
won't need all of it. Even if your dataset has many variables, for most
analyses you will find that you are only interested in some of them.
There are statistical reasons not to include twenty or more variables in
a data analysis as well. At a certain point, adding more variables
willÂ *appear*Â to make your analysis more accurate, but will in fact
obscure the truth from you. It's generally a good practice to select a
few variables you are interested in and go from there. As we discussed
above, the way to do this is to start with the research questions you
are trying to answer.

I'll use the same pre

```{r}
ds_predictors <- sci_mo_with_text %>%
  select(int,
        uv,
        pc,
        time_spent,
        final_grade,
        subject,
        enrollment_reason,
        semester,
        enrollment_status,
        cogproc,
        social,
        posemo,
        negemo,
        n
)
```

#### [**Your Turn**]{style="color: green;"} **â¤µ**

Now use the code chunk below to take a look at the data you just
imported using your preferred methods.

```{r}
my_predictors <- sci_mo_with_text %>%
  select(int,
        time_spent,
        final_grade,
        subject,
        enrollment_reason,
        semester,
        enrollment_status,
        cogproc,
        social,
        posemo,
        negemo,
        n
)
```

### c. Remove Missing Data

Since the machine learning model we will be using later in this case
study does not accept missing values

#### `na_omit()`

There are several different ways to remove missing values in R,
including the built-in function `na.omit()` which will remove any
observations, or cases, with missing values for any any of the
variables.

Run the code chunk below to see what happens if we try to remove missing
values using our the entire dataset we originally imported.

```{r}
df <-  na.omit(sci_mo_with_text)

df
```

#### [**Your Turn**]{style="color: green;"} **â¤µ**

1.  How many observations did this return? Why do you think that is?

#### drop_na() function

Another

```{r}
ds_predictors_2 <- drop_na(ds_predictors)

ds_predictors_2
```

#### [**Your Turn**]{style="color: green;"} **â¤µ**

Use the following code chunk to ...

```{r}
my_predictors_complete <- drop_na(my_predictors)

my_predictors_complete
```

1.  How many observations did this return? Why do you think that is?

## 3. EXPLORE

### a. Check Variance

Since the machine learning model we will be using later in this case
study does not accept missing values

```{r}
nearZeroVar(ds_predictors_2, saveMetrics = TRUE)
```

```{r}
ds_predictors_3 <- ds_predictors_2 %>%
  select(-enrollment_status)
```

#### [**Your Turn**]{style="color: green;"} **â¤µ**

Now use the code chunk below to take a look at the data you just im

```{r}
nearZeroVar(my_predictors_complete, saveMetrics = TRUE)
```

### b. Convert Characters to Factors

Since the machine learning model we will be using later in this case
study does not accept missing values

Since the machine learning model we will be using later in this case
study does not accept missing values

```{r}
ds_predictors_4 <- ds_predictors_3 %>% 
    mutate_if(is.character, as.factor)
```

```{r}

```

#### [**Your Turn**]{style="color: green;"} **â¤µ**

Now use the code chunk below to take a look at the data you just im

```{r}

```

## 4. MODEL

### a. Partition Data

Intro

#### Create an training index

First, we set a seed to ensure the reproducibility of our data
partition. we create a new object called trainIndex that will take 80
percent of the data We add a new variable to our dataset, temporarily:

\# this will let us select our rows according to their row number

\# we populate the rows with the numbers 1:464, in order

```{r}

set.seed(2020)

trainIndex <- createDataPartition(ds_predictors_4$final_grade,
                                  p = .8,
                                  list = FALSE,
                                  times = 1)
```

arguments:

-   `ds_predictors_4$final_grade` selects our outcome that we are
    interested in predicting, in our case students' final course grades;
-   `p =` specifies what proportion of the data we want to be in the
    *training* partition; and,
-   `times =` can be used to create *multiple* train and test sets,
    something we will revisit later, but for now we'll just create 1
    train and test set.

#### Index our predictors data set

\# We add a new variable to our dataset, temporarily:

\# this will let us select our rows according to their row number

\# we populate the rows with the numbers 1:464, in order

```{r}
ds_predictors_4 <- ds_predictors_4 %>% 
  mutate(temp_id = 1:464)
```

Filter our training set

```{r}
ds_train <- ds_predictors_4 %>% 
    filter(temp_id %in% trainIndex)
```

```{r}

```

## 5. COMMUNICATE

The final(ish) step in our workflow/process is sharing the results of
analysis with wider audience. Krumm et al.Â (2018) have outline the
following 3-step process for communicating with education stakeholders
what you have learned through analysis:

1.  **Select**. Communicating what one has learned involves selecting
    among those analyses that are most important and most useful to an
    intended audience, as well as selecting a form for displaying that
    information, such as a graph or table in static or interactive form,
    i.e.Â a "data product."

2.  **Polish**. After creating initial versions of data products,
    research teams often spend time refining or polishing them, by
    adding or editing titles, labels, and notations and by working with
    colors and shapes to highlight key points.

3.  **Narrate**. Writing a narrative to accompany the data products
    involves, at a minimum, pairing a data product with its related
    research question, describing how best to interpret the data
    product, and explaining the ways in which the data product helps
    answer the research question.

For Unit 1 we will keep it simple. In the code chunk below, select a
chart, table or model created above (or create an entirely new one based
a new analysis) that you think an education stakeholder might find
interesting. Beneath the code chunk, write a very brief narrative to
accompany your narrative.

### My First Data Product (Change Me)

```{r}

```

### Congratulations!

You've completed the first case study! To "turn in" your work, you can
click the "Knit" icon at the top of the file, or click the dropdown
arrow next to it and select "Knit top HTML". This will create a report
in your Files pane that serves as a record of your completed assignment
and its output you can open or share.
