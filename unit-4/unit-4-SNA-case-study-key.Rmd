---
title: 'Unit 2 Case Study: Hashtag Common Core'
subtitle: "ECI 586 Introduction to Learning Analytics"
author: "Dr. Shaun Kellogg"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. PREPARE

Our Unit 4 Case Study: Hashtag Common Core is inspired by the work of
Jonathan Supovitz, Alan Daly, Miguel del Fresno and Christian Kolouch
who examined the intense debate surrounding the Common Core State
Standards education reform as it played out on Twitter. As noted on
their expansive and interactive website for the [\#COMMONCORE
Project](https://www.hashtagcommoncore.com), the Common Core was a major
education policy initiative of the early 21st century. A primary aim of
the Common Core was to strengthen education systems across the United
States through a set of specific and challenging education standards.
Although these standards once enjoyed bipartisan support, we saw in our
previous case study how these standards have become a political punching
bag.

In Unit 4, we continue our investigation of tweets around the these
controversial state standards through social network analysis.
Specifically, this case study will cover the following topics pertaining
to each data-intensive workflow process:

1.  **Prepare**: Prior to analysis, we'll take a look at the context
    from which our data came, formulate some research questions, and get
    introduced the {tidygraph} and {ggraph} packages for analyzing
    relational data.

2.  **Wrangle**: In the wrangling section of our case study, we will
    learn some basic techniques for manipulating, cleaning,
    transforming, and merging network data.

3.  **Explore**: With our network data cleaned and tidied, we learn to
    calculate some key network measures and to illustrate some of these
    stats through network visualization.

4.  **Model**: We conclude our analysis by introducing community
    detection algorithms for identifying groups and gauging sentiment
    about the common core.

5.  **Communicate**: We briefly reflect on our walkthrough in
    preparation for our independent analysis next week.

## 1a. Review the Research

Recall from [Social Network Analysis and Education: Theory, Methods &
Applications](https://methods.sagepub.com/book/social-network-analysis-and-education)
that Carolyn (2013) cited the following four features used by Freeman
(2004) to define the social network perspective:

1.  Social network analysis is **motivated by a relational intuition**
    based on ties connecting social actors.

2.  It is firmly **grounded in systematic empirical data**.

3.  It **makes** **use of graphic imagery** to represent actors and
    their relations with one another.

4.  It **relies** **on** **mathematical and/or computational models** to
    succinctly represent the complexity of social life.

The [\#COMMONCORE Project](https://www.hashtagcommoncore.com) that we'll
examine next is an exemplary illustration of these four defining
features of the social network perspective.

### The \#commoncore Project

![](img/commoncore.jpeg){width="40%"}

Supovitz, J., Daly, A.J., del Fresno, M., & Kolouch, C.
(2017).Â *\#commoncore Project.* Retrieved from
[http://www.hashtagcommoncore.com](http://www.hashtagcommoncore.com/).

#### Prologue

As noted by @supovitz2017commoncore, the Common Core State Standards
have been a "persistent flashpoint in the debate over the direction of
American education." The \#commoncore Project explores the Common Core
debate on Twitter using a combination of social network analyses and
psychological investigations which help to reveal both the underlying
social structure of the conversation and the motivations of the
participants.

The central question guiding this investigation was:

> How are social media-enabled social networks changing the discourse in
> American politics that produces and sustains education policy?

#### Data Sources & Analyses

The [methods
page](https://www.hashtagcommoncore.com/project/methodology) of the
\#COMMONCORE Project provides a detailed discussion of the data and
analyses used to arrive at the conclusions in *\#commoncore: How social
media is changing the politics of education*. Provided below is a
summary of how authors retrieved data from Twitter and the analyses
applied for each of the five acts in the website. I highly encourage you
to take a look at this section if you'd like to learn more about their
approach and in particular if you're unfamiliar with how users can
interact and communicate on Twitter.

#### Data Collection

To collect data on keywords related to the Common Core, the project used
a customized data collection tool developed by two of our co-authors,
Miguel del Fresno and Alan J. Daly, calledÂ *Social Runner
Lab^TM^*.Â Similar to an approach we'll used in Unit 3, the authors
downloaded data in real time directly from Twitter's Application
Programming Interface (API) based on tweets using specified keywords,
keyphrases, or hashtags and then restricted their analysis to the
following terms: *commoncore*, *ccss* and *stopcommoncore.* They also
captured Twitter profile names, or user names, as well as the tweets,
retweets, and mentions posted. Data included messages that are public on
twitter, but not private direct messages between individuals, nor from
accounts which users have made private.

#### Analyses

In order to address their research question, the authors applied social
network analysis techniques in addition to qualitative and automated
text mining approaches. For social network analyses, each node is an
individual Twitter user (person, group, institution, etc.) and the
connection between each node is the tweet, retweet, or mention/reply.
After retrieving data from the Twitter API, the authors created a file
that could be analyzed in Gephi, an open-source software program which
depicts the relations as networks and provides metrics for describing
features of the network.

In addition to data visualization and network descriptives, the authors
examined group development and lexical tendencies among users. For group
development, they used a community detection algorithm to identify and
represent structural sub-communities, or "factions", which they describe
as a group with more ties within than across group even those group
boundaries are somewhat porous). For lexical tendencies, the authors
used the Linguistic Inquiry and Word Count (LIWC) lexicons to determine
psychological drive, diagnose their level of conviction, make inferences
about thinking styles, and even determine sentiment similar to what we
examined in Unit 2.

For a nice summary of the data used for the analysis, as well as the
samples of actors and tweets, the keywords, and the methods that were
utilized, see [Table 1. Data and Method for Each
Act](https://www.hashtagcommoncore.com/project/methodology) in the
Methods section of the \#commoncore website.

#### Key Findings

In the \#commoncore Project, analyses of almost 1 million tweets sent by
about 190,000 distinct actors over a period of 32 months revealed the
following:

-   In **Act 1**, **The Giant Network**, the authors identified five
    major sub-communities, or factions, in the Twitter debate
    surrounding the Common Core, including: (1) supporters of the Common
    Core, (2) opponents of the standards from inside education, and (3)
    opponents from outside of education.
-   In **Act 2**, **Central Actors**, they noted that most of these
    participants were casual contributors -- almost 95% of them made
    fewer than 10 tweets in any given six-month period. They also
    distinguished between two types of influence on
    Twitter:Â *Transmitters*Â who tweeted a lot, regardless of the extent
    of their followership; andÂ *Transceivers*, those who gained their
    influence by being frequently retweeted and mentioned.
-   In **Act 3**, **Key Events,**Â the authors identified issues driving
    the major spikes in the conversation, like when Secretary of
    Education Duncan spoke about white suburban moms' opposition to the
    Common Core, or the debate over the authorization of the Every
    Student Succeeds Act in November 2015. They also offended evidence
    of manufactured controversies spurred by sensationalizing minor
    issues and outright fake news stories.
-   In **Act 4**, **Lexical Tendencies**, the authors examined the
    linguistic tendencies of the three major factions and found that
    Common Core supporters used the highest number of conviction words,
    tended to use more achievement-oriented language, and used more
    words associated with a formal and analytic thinking style. By
    contrast, opponents of the Common Core from within education tended
    to use more words associated with sadness, and used more narrative
    thinking style language. Opponents of the Common Core from outside
    of education made the highest use of words associated with peer
    affiliation, used the largest number of angry words, and exhibited
    the lowest level of conviction in their word choices.
-   In **Act 5**, **The Tweet Machine**, examined five frames that
    opponents of the Common Core used to appeal to values of particular
    subgroups including the government frame, business frame, war frame,
    experiment frame, and propaganda frame. By combining these
    constituencies, the authors illustrated how the Common Core
    developed a strong transpartisan coalition of opposition.

### **ðŸ‘‰ Your Turn** **â¤µ**

For our Unit 2 Walkthrough, we'll apply some of the same techniques used
by this study including calculating some basic network measures. For
example, take a quick look at the [*Explore the
Networks*](https://www.hashtagcommoncore.com/#2-1){style="font-size: 10pt;"}
section from Act 2: Central Actors and the Transmitters, Transceivers
and Transcenders. In the space below, list a couple Twitter users
identified by their analysis as.

Visit

-   ***Transmitters*** .

-   ***Transceivers*** are a different kind of elite influencer.
    Transceivers are those actors who have what social network
    researchers call highÂ *indegree*. In our analyses, indegree is the
    combination of the number of times an actor's messages were
    retweeted, coupled with the number of times in which they are
    mentioned in others' tweets within the specified keywords. Mentions
    are signifiers of a different kind of influence in the \#commoncore
    conversation.

-   ***Transcenders***Â who have both high outdegree, defined as sending
    the largest number of common core-related tweets to keywords of
    interest,Â *as well as having* high indegree, defined as a
    combination of being retweeted and mentioned in the highest number
    of tweets. These individuals reflect those elite actors who possess
    the highest relative levels of activity within the network and wield
    a significant amount of social influence.

Now visit the methods In the space below, type

-   ***Transmitters*** are individuals who send out a large number of
    tweets using the keywords of interest. Social network researchers
    call the activity of transmittersÂ *outdegree*, which is a measure of
    the number of tweets an individual sends over the period of time
    under study. Outdegree isÂ *not* related to the number of followers a
    transmitter has, but is strictly a measure of how many tweets an
    individual posts to the specified keywords.

-   ***Transceivers*** are a different kind of elite influencer.
    Transceivers are those actors who have what social network
    researchers call highÂ *indegree*. In our analyses, indegree is the
    combination of the number of times an actor's messages were
    retweeted, coupled with the number of times in which they are
    mentioned in others' tweets within the specified keywords. Mentions
    are signifiers of a different kind of influence in the \#commoncore
    conversation.

-   ***Transcenders*** who have both high outdegree, defined as sending
    the largest number of common core-related tweets to keywords of
    interest,Â *as well as having* high indegree, defined as a
    combination of being retweeted and mentioned in the highest number
    of tweets. These individuals reflect those elite actors who possess
    the highest relative levels of activity within the network and wield
    a significant amount of social influence.

## 1b. Identify a Question(s)

Recall from above that the central question guiding the \#COMMONCORE
Project was:

> How are social media-enabled social networks changing the discourse in
> American politics that produces and sustains education policy?

For Unit 2, we are going to focus our questions on something a bit less
ambitious but inspired by this work:

1.  Who are the transmitters, transceivers, and transcenders in our
    Common Core Twitter network?
2.  What subgroups, or factions, exist in our network?
3.  Which actors in our network tend to be more opposed to the Common
    Core?

To address the latter question, we'll revisit our findings from our Unit
3 VADER sentiment analysis results.

### **ðŸ‘‰ Your Turn** **â¤µ**

Based on what you know about networks and the context so far, what other
research question(s) might ask we ask in this context that a social
network perspective might be able to answer?

In the space below, type a brief response to the following questions:

-   YOUR RESPONSE HERE

## **1c. Set Up Project**

As highlighted inÂ [Chapter 6 of Data Science in Education Using
R](https://datascienceineducation.com/c06.html)Â (DSIEUR), one of the
first steps of every workflow should be to set up your "Project" within
RStudio. Recall that:

> A **Project** is the home for all of the files, images, reports, and
> code that are used in any given project

Since we are working in RStudio Cloud, a Project has already been set up
for you as indicated by the `eci-589.Rproj` file in your main directory
in the Files pane.

### Load Libraries

In Unit 1, we also learned about **packages**, or libraries, which are
shareable collections of R code that can contain functions, data, and/or
documentation and extend the functionality of R. You can always check to
see which packages have already been installed and loaded into RStudio
Cloud by looking at the the Files, Plots, & Packages Pane in the lower
right hand corner.

### **ðŸ‘‰ Your Turn** **â¤µ**

First, load the {tidyverse} since we'll be using several of these
packages for to wrangling and explore our data later sections of the
case study:

```{r}
library(tidyverse)
```

### tidygraph ðŸ“¦

![](img/tidygraph.png){width="20%"}

The {[tidygraph](https://tidygraph.data-imaginist.com)} package is a
huge package that exports 280 different functions and methods, including
access to almost all of theÂ `dplyr`Â verbs plus a few more, developed for
use with relational data. While network data itself is not tidy, it can
be envisioned as two tidy tables, one for node data and one for edge
data. The {tidygraph} package provides a way to switch between the two
tables and uses `dplyr` verbs to manipulate them. Furthermore it
provides access to a lot of graph algorithms with return values that
facilitate their use in a tidy workflow.

Let's go ahead and load the {tidygraph} library:

```{r}
library(tidygraph)
```

### ggraph ðŸ“¦

![](img/ggraph.png){width="20%"}

Created by the same developer as {tidygraph},
{[ggraph](https://ggraph.data-imaginist.com/index.html)} -- pronounced
gg-raph or g-giraffe hence the logo -- is an extension of
{[ggplot](https://ggplot2.tidyverse.org)} aimed at supporting relational
data structures such as networks, graphs, and trees. Both packages are
more modern and widely adopted approaches data visualization in R.

While ggraph builds upon the foundation of ggplot and its API, it comes
with its own self-contained set of geoms, facets, etc., as well as
adding the concept ofÂ *layouts*Â to the [grammar of
graphics](https://ggplot2-book.org/introduction.html?q=grammar#what-is-the-grammar-of-graphics),
i.e. the "gg" in ggplot and ggraph.

Let's go ahead and load the {ggraph} library:

```{r}
library(ggraph)
```

### Import Data

Finally, use the `read_csv()` function from the {readr} package to read
the `ccss-tweets.csv` file from Unit 2 and saved in our data folder:

```{r}
ccss_tweets <- read_csv("data/ccss-tweets.csv")
```

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning,
reshaping, transforming, and merging data [@wickham2016r]. The
importance of data wrangling is difficult to overstate, as it involves
the initial steps of going from the raw data to a dataset that can be
explored and modeled [@krumm2018].

For our data wrangling this week, we're keeping it simple since working
with network data is a bit of a departure from our working with
rectangular data frames. Our primary goals for Unit 1 are learning how
to:

a.  **Import Tweets**. In this section, we introduce
    theÂ `rtweet`Â package and some key functions to search for tweets or
    users of interest.

b.  **Create a Network Object**. Before performing network analyses,
    we'll need to convert our data frames into special data format for
    working with relational data.

c.  **Simplify Network**. Finally, we'll learn about a handy
    `simplify()` function in the {igraph} package for collapsing
    multiple ties between actors and removing "self-loops."

## 2b. Create Edgelist from Tweets

Recall from Chapter 1 of @carolan2014 that ties, our relations, are what
connect actors to one another. These ties are often referred to as
"edges" when working with network data and range ofÂ edge types in a
network can be extensive. Some of the more common ties, or edges, used
to denote connections among actors in educational research include:

-   Behavioral interaction (e.g., talking to each other or sending
    messages)

-   Physical connection (e.g., sitting together at lunch, living in the
    same neighborhood)

-   Association or affiliation (e.g., taking the same courses, belonging
    to the same peer group)

-   Evaluation of one person by another (e.g., considering someone a
    friend or enemy)

-   Formal relations (e.g., knowing who has authority over whom)

-   Moving between places or status (e.g., school choice preferences,
    dating patterns among adolescents)

The ties on which you focus are driven by theoretical and/or empirical
interest. But defining what constitutes aÂ tieÂ is a thorny methodological
issue discussed in Chapter 3.

As its name implies, the first fileÂ `dlt1-edgelist.csv`Â is an edge-list
that contains information about each tie, or relation between two actors
in a network. In this context, a "tie" is a reply by one participant in
the discussion forum to the post of another participant -- or in some
cases to their own post! These ties between a single actor are called
"self-loops" and as we'll see later in this section, igraph has a
special function to remove these self loops from a sociogram, or network
visualization.

The edge-list format is slightly different than other formats you have
likely worked with before in that the values in the first two columns
each row represent a dyad, or tie between two nodes in a network. An
edge-list can also contain other information regarding the strength,
duration, or frequency of the relationship, sometime called "weight", in
addition to other "edge attributes."

Recall from Unit 1 that an edge-list that contains information about
each tie, or relation between two actors in a network. In our previous
walkthrough, a "tie" is a reply by one participant in the discussion
forum to the post of another participant -- or in some cases to their
own post. For our analysis of tweets, we'll use the same approach used
by \@supovitz2017commoncore. Specifically, each node is an individual
Twitter user (person, group, institution, etc.) and the connection
between each node is the tweet, retweet, or mention/reply.

### **ðŸ‘‰ Your Turn** **â¤µ**

As highlighted in @estrellado2020e, cleaning and tidying network data
can be even more challenging than for other data sources, network data
often includes variables about both individuals and their relationships.
This is especially true of our data downloaded using the Twitter API.

Take a look at the data we just imported using one of your favorite
methods for inspecting data and in the space below, identify the columns
you think could be used to construct an edge list.

-   YOUR RESPONSE HERE

### Extract Edges and Nodes

If one of the columns you indicated in your response above included
`screen_name` nice work! You may also have noticed that the
`mentions_screen_name` column also includes the names of those in the
reply column.

#### Create Edgelist

Recall from Unit 1 that the first two columns in an edgelist should
include the nodes that make up a tie or dyad. Since the only two columns
we need to construct our edgelist is the `screen_name` of the tweet
author and the screen names of those included in the mentions, let's
`relocate()` and rename those columns and then `select()` those columns
along with any attributes that we think might be useful for analysis
later on, like the timestamp or content of the tweet, i.e. `text`.

```{r}
ties_1 <-  ccss_tweets_5 %>%
  relocate(sender = screen_name, # rename scree_name to sender
           target = mentions_screen_name) %>% # rename to receiver
  select(sender,
         target,
         created_at,
         text)
```

Our edgelist with attributes is almost ready, but we have a couple
issues we still need to deal with.

As you may have noticed, our receiver column contains the names of
multiple Twitter users, but a dyad or tie can only be between two actors
or nodes.

#### "Unnest" User Names

In order to place each target user in a separate row that corresponds
with each sender of the tweet, we will need to "unnest" these names
using a package that those who took the ECI 588 Text Mining course will
be very familiar with.

#### tidytext ðŸ“¦

![](img/tidytext.png){width="20%"}

The {tidytext} package helps to convert text into data frames of
individual words, making it easy to to manipulate, summarize, and
visualize text using using familiar functions form the {tidyverse}
collection of packages.

Let's go ahead and load the {tidytext} package:

```{r}
library(tidytext)
```

For a more comprehensive introduction to the `tidytext` package, we
cannot recommend enough the free online book, *Text Mining with R: A
Tidy Approach* [@silge2017text]. If you're interested in pursuing text
analysis beyond this walkthrough, this will be a go to reference.

Let's go ahead and apply the \`un

```{r}
ties_2 <- ties_1 %>%
  unnest_tokens(input = target,
                output = receiver,
                to_lower = FALSE) %>%
  relocate(sender, receiver)
```

There is A LOT to unpack with this function. First notice
thatÂ `unnest_tokens`Â expects a data frame as the first argument, which
we supplied using teh %\>% operator, followed by two column names. The
first is an input column that the text comes from, `target` in this
case, which we want to "unnest" into separate rows. Also notice:

-   Other columns, such asÂ `sender`Â andÂ `text`, are retained.

-   By default, words are changed to lowercase, which makes them easier
    to compare or combine with other datasets. However, we can set
    theÂ `to_lower = FALSE`Â argument to turn off this behavior since
    capitalization is important for our user names.

Finally, we'll introduce the `drop_na()` function to remove the rows
with missing values from our `receiver` column since they are incomplete
dyads. We'll include these actors in our network later as part of our
nodelist in just a bit.

Let's drop our missing values and save as `ties` for our final data
frame

```{r}
ties <- ties_2 %>%
  drop_na(receiver)
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a quick look at our final edgelist and answer the following
question:

How many edges our in our CCSS network?

-   YOUR RESPONSE HERE

Using the tweets you downloaded with your own custom query from above,
create an edgelist with any desired edge attributes.

```{r}
# YOUR CODE HERE
```

### Node Attributes

The second file we need to create is a data frame that contains all the
nodes or actors in our network.

Regardless, let's read in our node attribute file and take a look at the
`actors` and their attributes included in our dataset:

```{r}
actors_1 <- ties_2 %>%
  pivot_longer(cols = sender:receiver, 
               names_to = "nodes",
               values_to = "screen_name")
```

Since

```{r}
actors <- actors_1 %>%
  select(screen_name) %>%
  distinct() %>%
  drop_na()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a quick look at our final `actors` data frame and answer the
following question:

How many nodes our in our CCSS network?

-   YOUR RESPONSE HERE

Using the tweets you downloaded with your own custom query from above,
create an nodelist containing all the unique actors in your network.

```{r}
# YOUR CODE HERE
```

## 2c. Create Network Object

In Unit 1 we learned about the {igraph} package for preparing network
objects for analysis. In this section we introduce a new package that
builds upon the "the well-oiled machinery of igraph" but allows us to
use some of the familiar functions and syntax from other {tidyverse}
packages. In this walkthrough we won't recreate all the processes like
simplifying graphs and adding edge weights, but will demonstrate some
features similar to and in addition to those found in the {igraph}
package.

### Combine Edges & Nodes

Before we can begin using many of the functions from the {tidygraph}
package for preparing and summarizing our Twitter network, we first need
to convert the data frames that we imported into a network object,
similar to what we did in Unit 1.

To do that, we will use the `tbl_graph()` function and include the
following arguments:

-   `edges =` expects a data frame, in our case `ties`, containing
    information about the edges in the graph. The nodes of each edge
    must either be in a `to` and `from` column, or in the two first
    columns like the data frame we provided.

-   `nodes =` expects a data frame, in our case `actors`, containing
    information about the nodes in the graph. If `to` and/or `from` are
    characters or names, like in our data frames, then they will be
    matched to the column named according to `node_key` in nodes, if it
    exists, or matched to the first column in the node list.

-   `directed =` specifies whether the constructed graph be directed and
    defaults to `TRUE` so we did not included it since our network is
    directed.

Let's go ahead and create our network graph, name it `network` and print
the output:

```{r}
network <- tbl_graph(edges = ties, 
                     nodes = actors)

network
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a look at the output for our simple graph now and answer the
following questions:

1.  Are the numbers and names of nodes and actors consistent with our
    `actors` and `ties` data frames? What about the integers included in
    the `from` and `to` columns of the Edge Data?

    -   YOUR RESPONSE HERE

2.  What do you think "components" refers to? **Hint:** see Chapter 6 of
    [@carolan2014].

    -   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of data wrangling section and are ready
to start analysis! Before proceeding further, knit your document and
check to see if you encounter any errors.

------------------------------------------------------------------------

# 3. EXPLORE

As noted in the Getting Started Walkthrough and experienced in Unit 1,
exploratory data analysis involves the processes of describing your data
(such as by calculating the means and standard deviations of numeric
variables, or counting the frequency of categorical variables) and,
often, visualizing your data prior to modeling.

In Section 3, we use the {tidygraph} package for retrieving network
descriptives and introduce the {ggraph} package to create a network
visualization to help illustrate these metrics. Specifically, in this
section we'll learn to:

a.  **Examine Basic Descriptives**. We focus primarily on actors and
    edges in this walkthrough, including the edges wights we added in
    the previous section as well as node degree, and import and fairly
    intuitive measure of centrality.

b.  **Make a Sociogram**. Finally, we wrap up the explore phases by
    learning to plot a network and tweak key elements like the size,
    shape, and position of nodes and edges to better at communicating
    key findings.

## 3a. Examine Basic Descriptives

As we noted in Unit 1, many analyses of social networks are primarily
descriptive and aim to either represent the network's underlying social
structure through data-reduction techniques or to characterize network
properties through network measures.

### Centrality

#### Node Degree

Recall from Unit 1 that:

> **Degree** is the number of ties to and from an ego. In a directed
> network, in-degree is the number of ties received, whereas out-degree
> is the number of ties sent.

The {tidygraph} package has an unique function called `activate()` that
allows us to treat the nodes in our network object as if they were a
standard data frame that we can then apply standard tidyverse functions
to like `select()`, `filter()`, and `mutate()`.

The latter function, `mutate()`, we can use to create new variables for
nodes such as measures of degree, in-degree, and out-degree using the
`centrality_degree()` function in the {tidygraph} package.

Run the following code to add degree measures to each of our nodes and
print the output:

```{r}
network_1 <- network %>%
  activate(nodes) %>%
  mutate(degree = centrality_degree(mode = "all")) %>%
  mutate(in_degree = centrality_degree(mode = "in")) %>%
  mutate(out_degree = centrality_degree(mode = "out"))

network_1
```

We now see that these simple measures of centrality have been added to
the nodes in our network.

We can also use the `activate()` function combined with the
`data.frame()` function to extract our new measures to a separate data
frame so we inspect our nodes individually and create some summary
statistics using the handy `summary()` function.

```{r}
node_measures <- network_1 %>% 
  activate(nodes) %>%
  data.frame()

summary(node_measures)
```

Despite a dramatic size difference from our network in Unit 1, we see
that typical nodes in this network also have relatively few connections,
though there are a few exceptions.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that one of our questions guiding this
analysis was:

> Who are the transmitters, transceivers, and transcenders in our Common
> Core Twitter network?

Use the code chunk below to inspect our `node_measures` data frame and
answer the questions above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

#### Other Centrality Measures

In Chapter 7 of @carolan2014, noted that degree centrality does not take
into account indirect ties among all the alters in an ego's network.Â We
were also introduced to a few other measures of centrality commonly used
in network analysis and applied to educational contexts:

-   **Closeness** includes data about the relation between each pair of
    ego's named alters and is intuitively appealing in that being
    "close" to others and may indicate how quickly an actor can exchange
    something with others or be the first to receive information.

-   **Betweenness** captures how actors control or mediate the relations
    between pairs of actors that are not directly connected and is an
    important indicator of control over information exchange or resource
    flows within a network.

Carolyn also notes that in addition to these three common centrality
measures, many others have been developed and can be calculated in most
common social network-analysis software applications.Â In fact,
{tidygraph} includes many of these measures and includes various
`centrality_` functions for calculating node and edge centrality. Type
`?centrality` in your console below and hit enter to view them all.

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the code chunk below to add these closeness and betweenness measures
to our `network_1` data frame and save as `network_2`. I've included
some basic code to get your started.

```{r}
network_2 <- network_1
  #YOUR CODE HERE
  
```

## 3b. Make a Sociogram

If you recall from Unit 1, network visualization can be used for a
variety of purposes, ranging from highlighting key actors to even
serving as works of art. These visual representations of the actors and
their relations, i.e. the network, are called a **sociogram**. Actors
who are most central to the network, such as those with higher node
degrees, are usually placed in the center of the sociogram and their
ties are placed near them. In this section, we'll briefly introduce the
{ggraph} package for creating attractive network visualizations.

### A Simple Sociogram

Very similar to how functions in the tidyverse use the `%>%` operator to
"pipe" functions together and progressively wrangle data, ggraph and
ggrplot use the `+` operator to "layer" functions together to
progressively build graphs.

Let's start with the first and simplest function `ggraph` and supply our
network graph:

```{r}
ggraph(network_1)
```

As you can see, this didn't produce much. All this function does is take
care of setting up the network object to plot along with creating the
layout for the plot based on the graph and the specified layout passed
in.

#### Add Layout

Let's go ahead and include the layout argument, which in addition to
including its own unique layouts, can incorporate layouts form {igraph}
like `fr`.

```{r}
ggraph(network_1, layout = "fr")
```

#### Add Nodes

Still nothing but that is because we haven't added the nodes yet. Let's
do that:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point() 
```

Well, at least we have our nodes now!

The "geom" in the `geom_non_point()` functions stands for "Geometric
elements", or geoms for short, and represent what you actually see in
the plot.

These geoms can include aesthetics, or aes for short, such as `alpha`
for transparency, as well as `colour`, `shape` and `size`.

Let's now add some "aesthetics" to our points by including the `aes()`
function and arguments such as `size =` which we can set to our
`in_degree` measures:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point(aes(size = in_degree, 
                    alpha = out_degree, 
                    colour = degree))
```

And let's add some node text and labels while were at it since this is
not a very large network. Since node labels are a geometric element, we
can apply aesthetics to them as well. Let's also include the `repel =`
argument that when set toÂ `TRUE`Â will avoid overlapping text.

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = in_degree/2,
                     alpha = degree),
                 repel=TRUE)
```

Much better! Even without the edges, using size and opacity has helped
to illustrate some of the "transmitters," "trancievers" and
"transcenders."

#### Add Edges

Now, let's connect the dots and add some edges that include some arrows
1mm in length as well as an end cap to keep them from overlapping the
nodes:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3)
```

#### Add a Theme

Finally, let's add a **theme,** which controls the finer points of
display, like the font size and background color. The `theme_graph()`
function add a theme specially tuned for graph visualizations. This
function removes redundant elements in order to put focus on the data
and if you type `?theme_graph` in the console you will get a sense of
the level of fine tuning you can do if desired.

Let's add `theme_graph()` to our sociogram and call it good for now:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Try modifying the code below by tweaking the included function/arguments
or adding new ones for
[layouts](https://ggraph.data-imaginist.com/articles/Layouts.html),
[nodes](https://ggraph.data-imaginist.com/articles/Nodes.html), and
[edges](https://ggraph.data-imaginist.com/articles/Edges.html) to make
our plot either more "aesthetically pleasing" or more purposeful in what
it's trying to communicate.

There are no right or wrong answers, just have some fun trying out
different approaches!

```{r}
ggraph(network_1, layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of the Explore section and are ready to
learn a little about network modeling! Before proceeding further, knit
your document and check to see if you encounter any errors.

# 4. MODEL

As highlighted inÂ [Chapter 3 of Data Science in Education Using
R](https://datascienceineducation.com/c03.html), theÂ **Model**Â step of
the data science process entails "using statistical models, from simple
to complex, to understand trends and patterns in the data."

## 4a. Identify Groups

In Chapter 6: Groups and Positions in Complete Networks of SNA and
Education [@carolan2014] we were introduced to both "bottom up" and "top
down" approaches for identifying groups in a network, as well as why
researchers may be interested in exploring these groups. He also notes
that:

> Unlike most social science, the idea is to identify these groups
> through their relational data, not an exogenous attribute such as
> grade level, departmental affiliation, or years of experience.Â 

In this section, we'll briefly explore a "top down" approach to
identifying these groups through the use of community detection
algorithms.

### Community Detection

Similar to the range of functions included for calculating node and edge
centrality, the {tidygraph} package includes various clustering
functions provided by the {igraph} package introduced in Unit 1.

Also similar to calculating centrality measures, we need to `activate()`
our nodes first before applying these community detection algorithms to
assign our nodes to groups.

Run the following code and take a print our new network graph to the
console to take a quick look

```{r}
network_3 <- network_2 %>%
  activate(nodes) %>%
  mutate(group = group_infomap())
  
network_3
```

**Note:** Some of these algorithms are designed for directed graphs,
while others are for undirected graphs.

Now that we've assigned our nodes to a group, let's modify our sociogram
from above to color our nodes by group assignment and remove the
`alpha =` argument to make them a little easier to see.

```{r}
network_3 %>%
  ggraph(layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      colour = group)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the second question guiding this
analysis was:

> What subgroups, or factions, exist in our network?

Use the code chunk below to extract the group assignment data frame and
answer the question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

## 4b. Identify Sentiment

Sentiment analysis (also known asÂ opinion mining) is a text mining
technique used to "systematically identify, extract, quantify, and study
affective states and subjective information." In this section, we'll
introduce and apply the {vader} package to gain some insight into the
"lexical tendencies" of our tweets.

#### The vader Package ðŸ“¦

![](img/vader.jpeg){width="20%"}\

The {vader} package is for the Valence Aware Dictionary for sEntiment
Reasoning (VADER), a rule-based model for general sentiment analysis of
social media text and specifically attuned to measuring sentiment in
microblog-like contexts.

To learn more about the {vader} package and its development, take a look
at the article by Hutto and Gilbert (2014), [VADER: A Parsimonious
Rule-based Model forSentiment Analysis of Social Media
Text](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).

Let's go ahead and load the vader library:

```{r}
library(vader)
```

The {vader} package basically has just one function, `vader_df()` that
does one thing and expects just one column from a data frame. Let's give
VADER the our `ccss_tweets_6` data frame that we created earlier and
include the `$` operator to select for analysis our `text` column
containing our tweets.

```{r}
summary_vader <- vader_df(ties$text)

summary_vader
```

Hutto, C. & Gilbert, E. (2014) provide an excellent summary of the VADER
package on their [GitHub repository](scores) and I've copied and
explanation of the scores below:

-   TheÂ `compound`Â score is computed by summing the valence scores of
    each word in the lexicon, adjusted according to the rules, and then
    normalized to be between -1 (most extreme negative) and +1 (most
    extreme positive). This is the most useful metric if you want a
    single unidimensional measure of sentiment for a given sentence.
    Calling it a 'normalized, weighted composite score' is accurate.

**NOTE:**Â TheÂ `compound`Â score is the one most commonly used for
sentiment analysis by most researchers, including the authors.

Let's use the `inner_join()` function to add these sentiment scores
values back to our `ties` data frame and take a quick look:

```{r}
tweet_sentiment <-inner_join(summary_vader, 
                             ties,
                             by = "text")

tweet_sentiment
```

Now that we have these joined, we can take a quick look at the sentiment
.

```{r}
user_sentiment <- tweet_sentiment %>%
  group_by(sender) %>%
  summarise(sentiment = mean(compound))

user_sentiment
```

Note that we have effectively just created some new node and edge
"attributes" that could be incorporated into our network visualization
to potentially help understand why groups may have formed as they did.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the final question guiding this
analysis was:

> Which actors in our network tend to be more opposed to the Common
> Core?

Use the code chunk below to inspect our data frame and answer the
question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

------------------------------------------------------------------------

# 5. COMMUNICATE

For your Independent Analysis assignment for Unit 2 next week, you'll
create either a simple report or slide deck using an R Markdown document
just like this to share out some key findings from your analysis.
Regardless of whether you plan to talk us through your analysis and
findings with a presentation or walk us through with a brief written
report, your presentation or report should address the following
questions:

1.  **Purpose**. What question or questions are guiding your analysis?
    What did you hope to learn by answering these questions and why
    should your audience care about your findings?

2.  **Methods**. What data did you selected for analysis? What steps did
    you take took to prepare your data for analysis and what techniques
    you used to analyze your data? These should be fairly explicit with
    your embedded code.

3.  **Findings**. What did you ultimately find? How do your "data
    products" help to illustrate these findings? What conclusions can
    you draw from your analysis?

4.  **Discussion**. What were some of the strengths and weaknesses of
    your analysis? How might your audience use this information? How
    might you revisit or improve upon this analysis in the future?

### **ðŸ‘‰ Your Turn** **â¤µ**

Now that you've become more familiar with this dataset and the social
network perspective, what other aspects of this dataset, or a dataset
you are interested in exploring, could you investigate?

-   YOUR RESPONSE HERE

What specific research questions might you ask that would be helpful for
being understanding and improving learning, or the context in which the
data is collected? How could other approaches like sentiment analysis

-   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You've finished the Unit 2 Guided Walkthrough and are ready
for some independent analysis next week!

To complete this assignment, knit your document and send me an email at
sbkellog\@ncsu.edu letting me know you're all set.
